{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e066269",
   "metadata": {},
   "source": [
    "# Crypto Network Analysis + LLM-Augmented Pipeline (Starter Notebook)\n",
    "\n",
    "This notebook is a **drop-in scaffold** for your study:\n",
    "\n",
    "1. Load yearly transaction graphs (edge list CSV: `src,dst,weight` optional).\n",
    "2. Compute core metrics: degree distribution, top hubs, component sizes.\n",
    "3. **Entity-aware** metrics via an optional address→entity mapping CSV.\n",
    "4. Compare sampling policies (baseline vs. Random-Walk-with-Fly-Back, RWFB).\n",
    "5. Simple **attack simulations** (remove top-k nodes/entities, re-check robustness).\n",
    "6. Auto-generate a short **Results** text block you can paste into the paper.\n",
    "\n",
    "**Files expected** (you can replace the synthetic demo below):\n",
    "- `edges_YEAR.csv` with columns `src,dst` and optional `weight`.\n",
    "- `entity_map.csv` with columns `address,entity` (optional).\n",
    "\n",
    "---\n",
    "If you need a custom parser (e.g., from raw block explorer exports), add it where indicated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aad2cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports (pure-Python / stdlib + pandas/matplotlib only; no internet installs needed)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "\n",
    "pd.set_option('display.max_rows', 20)\n",
    "pd.set_option('display.width', 120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46c6a5f",
   "metadata": {},
   "source": [
    "## 0) Synthetic demo data (replace with your real edges/constants)\n",
    "This creates a small directed graph with a few hubs and community structure so all cells run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f212f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_synthetic_graph(n_communities=3, community_size=400, hub_prob=0.08, intra_p=0.01, inter_p=0.001, seed=7):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    nodes = []\n",
    "    for c in range(n_communities):\n",
    "        for i in range(community_size):\n",
    "            nodes.append(f\"c{c}_n{i}\")\n",
    "    hubs = set(random.sample(nodes, max(3, int(hub_prob*len(nodes)))))\n",
    "    edges = []\n",
    "    # intra-community edges\n",
    "    for c in range(n_communities):\n",
    "        comm_nodes = [n for n in nodes if n.startswith(f\"c{c}_\")]\n",
    "        for src in comm_nodes:\n",
    "            for dst in comm_nodes:\n",
    "                if src != dst and np.random.rand() < intra_p:\n",
    "                    edges.append((src, dst, 1.0))\n",
    "    # inter-community edges\n",
    "    for src in nodes:\n",
    "        for dst in nodes:\n",
    "            if src.split('_')[0] != dst.split('_')[0] and src != dst and np.random.rand() < inter_p:\n",
    "                edges.append((src, dst, 1.0))\n",
    "    # connect to hubs\n",
    "    for src in nodes:\n",
    "        for h in hubs:\n",
    "            if src != h and np.random.rand() < 0.03:\n",
    "                edges.append((src, h, 1.0))\n",
    "            if src != h and np.random.rand() < 0.02:\n",
    "                edges.append((h, src, 1.0))\n",
    "    df = pd.DataFrame(edges, columns=[\"src\",\"dst\",\"weight\"])\n",
    "    return df, nodes, hubs\n",
    "\n",
    "edges_df, nodes, hubs = make_synthetic_graph()\n",
    "display(edges_df.head())\n",
    "print(\"Nodes:\", len(nodes), \"Edges:\", len(edges_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0ea5a5",
   "metadata": {},
   "source": [
    "## 1) Core utilities: graph build, degree stats, components, RWFB sampling\n",
    "We avoid heavyweight graph libs for portability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8998958",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_adj(edges: pd.DataFrame, directed=True):\n",
    "    adj = defaultdict(list)\n",
    "    for _, row in edges.iterrows():\n",
    "        adj[row['src']].append((row['dst'], row.get('weight', 1.0)))\n",
    "        if not directed:\n",
    "            adj[row['dst']].append((row['src'], row.get('weight', 1.0)))\n",
    "    return adj\n",
    "\n",
    "def degree_stats(edges: pd.DataFrame):\n",
    "    out_deg = edges.groupby('src').size()\n",
    "    in_deg = edges.groupby('dst').size()\n",
    "    all_nodes = set(out_deg.index) | set(in_deg.index)\n",
    "    out_deg = out_deg.reindex(all_nodes).fillna(0).astype(int)\n",
    "    in_deg = in_deg.reindex(all_nodes).fillna(0).astype(int)\n",
    "    deg = out_deg + in_deg\n",
    "    return pd.DataFrame({\"in_deg\": in_deg, \"out_deg\": out_deg, \"deg\": deg}).sort_values('deg', ascending=False)\n",
    "\n",
    "class DSU:\n",
    "    def __init__(self):\n",
    "        self.p = {}\n",
    "        self.sz = {}\n",
    "    def find(self, x):\n",
    "        if x not in self.p:\n",
    "            self.p[x] = x; self.sz[x] = 1\n",
    "        while x != self.p[x]:\n",
    "            self.p[x] = self.p[self.p[x]]\n",
    "            x = self.p[x]\n",
    "        return x\n",
    "    def union(self, a,b):\n",
    "        ra, rb = self.find(a), self.find(b)\n",
    "        if ra == rb: return\n",
    "        if self.sz[ra] < self.sz[rb]: ra, rb = rb, ra\n",
    "        self.p[rb] = ra\n",
    "        self.sz[ra] += self.sz[rb]\n",
    "\n",
    "def components_undirected(edges: pd.DataFrame):\n",
    "    dsu = DSU()\n",
    "    for _, r in edges.iterrows():\n",
    "        dsu.union(r['src'], r['dst'])\n",
    "    comp = defaultdict(list)\n",
    "    for n in set(edges['src']).union(set(edges['dst'])):\n",
    "        comp[dsu.find(n)].append(n)\n",
    "    sizes = sorted([len(v) for v in comp.values()], reverse=True)\n",
    "    return sizes, comp\n",
    "\n",
    "def rwfb_sample(adj, start=None, steps=10000, fly_back_p=0.3):\n",
    "    # Random Walk with Fly-Back over directed graph using out-edges.\n",
    "    nodes = list(adj.keys())\n",
    "    if start is None:\n",
    "        start = random.choice(nodes)\n",
    "    cur = start\n",
    "    visited = []mi g\n",
    "    for _ in range(steps):\n",
    "        visited.append(cur)\n",
    "        if random.random() < fly_back_p:\n",
    "            cur = start\n",
    "        else:\n",
    "            nbrs = [v for v,_ in adj.get(cur, [])]\n",
    "            if nbrs:\n",
    "                cur = random.choice(nbrs)\n",
    "            else:\n",
    "                cur = start\n",
    "    return Counter(visited)\n",
    "\n",
    "adj = to_adj(edges_df, directed=True)\n",
    "deg_df = degree_stats(edges_df)\n",
    "sizes, comp = components_undirected(edges_df)\n",
    "print(\"Top-degree nodes:\")\n",
    "display(deg_df.head(10))\n",
    "print(\"Top component sizes:\", sizes[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7bd349",
   "metadata": {},
   "source": [
    "### Plots: degree distribution (log-log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39709b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "deg_counts = deg_df['deg'].value_counts().sort_index()\n",
    "plt.figure()\n",
    "plt.scatter(deg_counts.index, deg_counts.values)\n",
    "plt.xscale('log'); plt.yscale('log')\n",
    "plt.title('Degree distribution (undirected degree)')\n",
    "plt.xlabel('Degree'); plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b166abf2",
   "metadata": {},
   "source": [
    "## 2) Entity-aware metrics (using optional address→entity map)\n",
    "Provide a CSV named `entity_map.csv` with columns `address,entity`.\n",
    "We'll compute: share of edges (by count and weight) captured by the top-k entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a81b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entity_metrics(edges: pd.DataFrame, entity_map: pd.DataFrame, topk=10):\n",
    "    m = dict(zip(entity_map['address'], entity_map['entity']))\n",
    "    edges2 = edges.copy()\n",
    "    edges2['src_entity'] = edges2['src'].map(m).fillna(edges2['src'])\n",
    "    edges2['dst_entity'] = edges2['dst'].map(m).fillna(edges2['dst'])\n",
    "    # collapse to entity-level graph\n",
    "    ent_edges = edges2.groupby(['src_entity','dst_entity'], as_index=False)['weight'].sum()\n",
    "    ent_edges['count'] = 1\n",
    "    # in/out degree at entity-level (by edge count)\n",
    "    out_deg_e = ent_edges.groupby('src_entity')['count'].sum()\n",
    "    in_deg_e = ent_edges.groupby('dst_entity')['count'].sum()\n",
    "    ent_nodes = set(out_deg_e.index) | set(in_deg_e.index)\n",
    "    out_deg_e = out_deg_e.reindex(ent_nodes).fillna(0).astype(int)\n",
    "    in_deg_e = in_deg_e.reindex(ent_nodes).fillna(0).astype(int)\n",
    "    deg_e = out_deg_e + in_deg_e\n",
    "    deg_table = pd.DataFrame({\"in\": in_deg_e, \"out\": out_deg_e, \"deg\": deg_e}).sort_values('deg', ascending=False)\n",
    "    # flow concentration by weight\n",
    "    total_w = ent_edges['weight'].sum()\n",
    "    ent_out_w = ent_edges.groupby('src_entity')['weight'].sum().sort_values(ascending=False)\n",
    "    ent_in_w  = ent_edges.groupby('dst_entity')['weight'].sum().sort_values(ascending=False)\n",
    "    topk_out = ent_out_w.head(topk).sum() / (total_w if total_w>0 else 1)\n",
    "    topk_in  = ent_in_w.head(topk).sum() / (total_w if total_w>0 else 1)\n",
    "    return deg_table, float(topk_out), float(topk_in), ent_edges\n",
    "\n",
    "# Demo: fabricate a tiny mapping where hubs share an entity name\n",
    "demo_map = pd.DataFrame({\n",
    "    'address': list(hubs)[:5],\n",
    "    'entity': [f'Exchange_{i}' for i in range(min(5, len(hubs)))]\n",
    "})\n",
    "deg_e, topk_out_share, topk_in_share, ent_edges = entity_metrics(edges_df, demo_map, topk=5)\n",
    "print('Entity-level top-degree:')\n",
    "display(deg_e.head(10))\n",
    "print('Top-5 entity out-flow share (by weight):', round(topk_out_share, 4))\n",
    "print('Top-5 entity in-flow share (by weight):', round(topk_in_share, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfefa830",
   "metadata": {},
   "source": [
    "## 3) Sampling policy comparison (RWFB vs baseline)\n",
    "We compare the degree distribution stability and hub overlap when sub-sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b474c0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_baseline(nodes, frac=0.3, seed=0):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    k = max(1, int(frac * len(nodes)))\n",
    "    return set(rng.choice(nodes, size=k, replace=False))\n",
    "\n",
    "def induced_subgraph(edges: pd.DataFrame, keep_nodes: set):\n",
    "    return edges[edges['src'].isin(keep_nodes) & edges['dst'].isin(keep_nodes)].copy()\n",
    "\n",
    "def hub_overlap(full_deg: pd.Series, sub_deg: pd.Series, k=20):\n",
    "    top_full = set(full_deg.sort_values(ascending=False).head(k).index)\n",
    "    top_sub  = set(sub_deg.sort_values(ascending=False).head(k).index)\n",
    "    return len(top_full & top_sub) / max(1, len(top_full))\n",
    "\n",
    "full_deg_series = degree_stats(edges_df)['deg']\n",
    "keep_nodes_base = sample_baseline(list(full_deg_series.index), frac=0.3, seed=1)\n",
    "edges_base = induced_subgraph(edges_df, keep_nodes_base)\n",
    "deg_base = degree_stats(edges_base)['deg'] if len(edges_base)>0 else pd.Series(dtype=int)\n",
    "overlap_base = hub_overlap(full_deg_series, deg_base) if not deg_base.empty else 0.0\n",
    "\n",
    "visits = rwfb_sample(adj, steps=5000, fly_back_p=0.3)\n",
    "keep_nodes_rw = set(visits.keys())\n",
    "edges_rw = induced_subgraph(edges_df, keep_nodes_rw)\n",
    "deg_rw = degree_stats(edges_rw)['deg'] if len(edges_rw)>0 else pd.Series(dtype=int)\n",
    "overlap_rw = hub_overlap(full_deg_series, deg_rw) if not deg_rw.empty else 0.0\n",
    "\n",
    "print('Hub overlap@20 (Baseline):', round(overlap_base,3))\n",
    "print('Hub overlap@20 (RWFB p=0.3):', round(overlap_rw,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a59b04",
   "metadata": {},
   "source": [
    "## 4) Attack simulations (remove top-k nodes/entities)\n",
    "We remove the top-k hubs or entities and recompute giant component size as a robustness proxy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98228f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nodes(edges: pd.DataFrame, bad_nodes: set):\n",
    "    m = ~edges['src'].isin(bad_nodes) & ~edges['dst'].isin(bad_nodes)\n",
    "    return edges[m].copy()\n",
    "\n",
    "def giant_component_size(edges: pd.DataFrame):\n",
    "    sizes, _ = components_undirected(edges)\n",
    "    return sizes[0] if sizes else 0\n",
    "\n",
    "k = 20\n",
    "top_nodes = list(deg_df.index[:k])\n",
    "edges_k = remove_nodes(edges_df, set(top_nodes))\n",
    "g0 = giant_component_size(edges_df)\n",
    "gk = giant_component_size(edges_k)\n",
    "print(f'Giant component | original: {g0} → after removing top-{k} nodes: {gk} (Δ={g0-gk})')\n",
    "\n",
    "# Entity-level removal demo using demo_map\n",
    "deg_e_table,_,_,ent_e = entity_metrics(edges_df, demo_map, topk=5)\n",
    "top_entities = set(deg_e_table.head(5).index)\n",
    "map_dict = dict(zip(demo_map['address'], demo_map['entity']))\n",
    "bad_nodes_e = {addr for addr,ent in map_dict.items() if ent in top_entities}\n",
    "edges_e_removed = remove_nodes(edges_df, bad_nodes_e)\n",
    "g_e = giant_component_size(edges_e_removed)\n",
    "print(f'Giant component | after removing top-5 entities (by mapped addresses): {g_e} (original {g0})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600bf766",
   "metadata": {},
   "source": [
    "## 5) Auto-generated Results blurb (paste into your paper)\n",
    "Edit the wording as needed; this is a minimal narrative tied to the computed numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11c29dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpl = []\n",
    "tmpl.append(f\"We analyze a directed transaction graph comprising {len(set(edges_df['src']).union(set(edges_df['dst'])))} addresses and {len(edges_df)} edges.\")\n",
    "tmpl.append(f\"The degree distribution exhibits a heavy-tailed shape; the top-10 nodes hold a median degree of {int(deg_df['deg'].head(10).median())}.\")\n",
    "tmpl.append(f\"The largest connected component contains {g0} nodes, indicating substantial connectivity.\")\n",
    "tmpl.append(f\"Under random removal of 30% of nodes (baseline subsampling), hub overlap@20 is {round(overlap_base,3)}; with RWFB (p=0.3), overlap@20 increases to {round(overlap_rw,3)}, suggesting improved stability of hub identification under walk-based sampling.\")\n",
    "tmpl.append(f\"Mapping a subset of high-degree addresses to entities yields a top-5 entity out-flow share of {round(topk_out_share,3)} and in-flow share of {round(topk_in_share,3)}; removing these entities reduces the giant component from {g0} to {g_e} nodes, quantifying concentration and systemic impact.\")\n",
    "results_blurb = \" \".join(tmpl)\n",
    "print(results_blurb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70201c18",
   "metadata": {},
   "source": [
    "## 6) Next steps / TODO hooks\n",
    "- Swap synthetic demo for your real `edges_YEAR.csv` and `entity_map.csv`.\n",
    "- Add richer centrality (betweenness/bridging) if you provide a smaller graph slice (since exact APSP is expensive without graph libs).\n",
    "- (Optional) Add **LLM orchestration**: have an agent write/run new sampling sweeps, sensitivity checks, and produce figure captions automatically.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
